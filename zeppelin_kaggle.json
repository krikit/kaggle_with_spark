{
  "paragraphs": [
    {
      "text": "%md\n\nKaggle 시작하기\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
      "dateUpdated": "Feb 26, 2016 12:49:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456328147262_-667790405",
      "id": "20160224-153547_524490756",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eKaggle 시작하기\u003c/h1\u003e\n"
      },
      "dateCreated": "Feb 24, 2016 3:35:47 PM",
      "dateStarted": "Feb 26, 2016 12:49:05 PM",
      "dateFinished": "Feb 26, 2016 12:49:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nKaggle 이란?\n------------\n\n\u003e [Kaggle](http://kaggle.com) is a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models. This crowdsourcing approach relies on the fact that there are countless strategies that can be applied to any predictive modelling task and it is impossible to know at the outset which technique or analyst will be most effective.  -- from Wikipedia\n\n* Netflix Prize란 사용자들의 영화 별점 데이터를 제공하고 기계학습을 통한 영화 평가 예측(추천) 대회입니다. 1등에게 주어진 상금이 무려 백만불이었다고 합니다. 이 대회를 통해 collaborative filtering 알고리즘이 비약적으로 발전하게 되었다고 합니다. Kaggle은 이 Netflix Prize에서 착안한 경연입니다.\n* Netflix Prize에 대한 보다 자세하고 재밌는 이야기는 [이곳](http://www.shalomeir.com/2014/11/netflix-prize-1/)을 참고하시기 바랍니다.",
      "dateUpdated": "Feb 26, 2016 12:49:05 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456328164853_1250983008",
      "id": "20160224-153604_1282590751",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eKaggle 이란?\u003c/h2\u003e\n\u003cblockquote\u003e\u003cp\u003e\u003ca href\u003d\"http://kaggle.com\"\u003eKaggle\u003c/a\u003e is a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models. This crowdsourcing approach relies on the fact that there are countless strategies that can be applied to any predictive modelling task and it is impossible to know at the outset which technique or analyst will be most effective.  \u0026ndash; from Wikipedia\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003eNetflix Prize란 사용자들의 영화 별점 데이터를 제공하고 기계학습을 통한 영화 평가 예측(추천) 대회입니다. 1등에게 주어진 상금이 무려 백만불이었다고 합니다. 이 대회를 통해 collaborative filtering 알고리즘이 비약적으로 발전하게 되었다고 합니다. Kaggle은 이 Netflix Prize에서 착안한 경연입니다.\u003c/li\u003e\n\u003cli\u003eNetflix Prize에 대한 보다 자세하고 재밌는 이야기는 \u003ca href\u003d\"http://www.shalomeir.com/2014/11/netflix-prize-1/\"\u003e이곳\u003c/a\u003e을 참고하시기 바랍니다.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 24, 2016 3:36:04 PM",
      "dateStarted": "Feb 26, 2016 12:49:09 PM",
      "dateFinished": "Feb 26, 2016 12:49:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nTitanic Competition\n-------------------\n\n* [Titanic Competetion](https://www.kaggle.com/c/titanic)은 Kaggle에서 제공하는 튜토리얼 성격의 competetion입니다. 실제 타이타닉이 침몰할 때의 승객 중 살아남은 사람과 죽은 사람 데이터를 이용하여, 일부를 학습 데이터로 제공하고 테스트 데이터에서 살아남은 사람을 예측하는 competition입니다.\n* 왼쪽 Dashboard 탭의 [Data](https://www.kaggle.com/c/titanic/data)에서 학습을 위한 데이터 train.csv와 테스트를 위한 데이터 test.csv를 받을 수 있습니다. train.csv에는 자질 정보와 함께 정답이 제공되며, test.csv에는 정답이 없는데 이 정답을 예측하는 것이 competetion의 목표입니다.\n* [Make a submission](https://www.kaggle.com/c/titanic/submissions/attach)에 포맷에 맞는 예측 값을 제출하면, [Learderboard](https://www.kaggle.com/c/titanic/leaderboard)에 제출한 답안에 대한 점수 및 등수가 표시됩니다.",
      "dateUpdated": "Feb 26, 2016 12:49:05 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456328303064_1689428188",
      "id": "20160224-153823_2133506643",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eTitanic Competition\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href\u003d\"https://www.kaggle.com/c/titanic\"\u003eTitanic Competetion\u003c/a\u003e은 Kaggle에서 제공하는 튜토리얼 성격의 competetion입니다. 실제 타이타닉이 침몰할 때의 승객 중 살아남은 사람과 죽은 사람 데이터를 이용하여, 일부를 학습 데이터로 제공하고 테스트 데이터에서 살아남은 사람을 예측하는 competition입니다.\u003c/li\u003e\n\u003cli\u003e왼쪽 Dashboard 탭의 \u003ca href\u003d\"https://www.kaggle.com/c/titanic/data\"\u003eData\u003c/a\u003e에서 학습을 위한 데이터 train.csv와 테스트를 위한 데이터 test.csv를 받을 수 있습니다. train.csv에는 자질 정보와 함께 정답이 제공되며, test.csv에는 정답이 없는데 이 정답을 예측하는 것이 competetion의 목표입니다.\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"https://www.kaggle.com/c/titanic/submissions/attach\"\u003eMake a submission\u003c/a\u003e에 포맷에 맞는 예측 값을 제출하면, \u003ca href\u003d\"https://www.kaggle.com/c/titanic/leaderboard\"\u003eLearderboard\u003c/a\u003e에 제출한 답안에 대한 점수 및 등수가 표시됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 24, 2016 3:38:23 PM",
      "dateStarted": "Feb 26, 2016 12:49:09 PM",
      "dateFinished": "Feb 26, 2016 12:49:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nPart 1: Gender Model\n--------------------\n\n* 탑승자의 성별만 가지고 생존 여부를 판단하는 간단한 모델입니다.",
      "dateUpdated": "Feb 26, 2016 12:49:05 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456331039923_-1253934606",
      "id": "20160224-162359_1052697881",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003ePart 1: Gender Model\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e탑승자의 성별만 가지고 생존 여부를 판단하는 간단한 모델입니다.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 24, 2016 4:23:59 PM",
      "dateStarted": "Feb 26, 2016 12:49:10 PM",
      "dateFinished": "Feb 26, 2016 12:49:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\n\n// 먼저 spark-csv를 사용하기 위해 dependency 설정을 해줍니다.\nz.reset()\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\nz.load(\"com.databricks:spark-csv_2.10:1.2.0\")",
      "dateUpdated": "Feb 26, 2016 12:49:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456447515027_-634158012",
      "id": "20160226-004515_566815075",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@433905c0\n"
      },
      "dateCreated": "Feb 26, 2016 12:45:15 AM",
      "dateStarted": "Feb 26, 2016 12:49:06 PM",
      "dateFinished": "Feb 26, 2016 12:49:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// spark-csv를 이용하여 train 데이터를 읽어들입니다.\nval train \u003d (sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")          // 첫번째 줄을 헤더로 인식합니다.\n    .option(\"inferSchema\", \"true\")     // 자동으로 데이터 타입을 추론합니다.\n    .load(\"/titanic/train.csv\"))\n\n// 읽어들인 데이터는 RDD가 아니라 DataFrame이라 임시 table로 등록하면 SQL 쿼리가 가능합니다.\ntrain.registerTempTable(\"train\")\n\n// 이것 저것 자주 사용할 것이므로 캐싱해 둡니다.\ntrain.cache",
      "dateUpdated": "Feb 26, 2016 12:49:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456447884680_937694487",
      "id": "20160226-005124_1110217245",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "train: org.apache.spark.sql.DataFrame \u003d [PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]\nres6: train.type \u003d [PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]\n"
      },
      "dateCreated": "Feb 26, 2016 12:51:24 AM",
      "dateStarted": "Feb 26, 2016 12:49:11 PM",
      "dateFinished": "Feb 26, 2016 12:50:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// 성별에 따라 살아남은 비율을 조사합니다.\nval survivedBySex \u003d (train.select(\"Sex\", \"Survived\")    // 두 컬럼만 선택하여 리스트를 만들어 줍니다.\n    .map(row \u003d\u003e ((row(0), row(1)), 1))                  // (성별, 생사)를 key로, 빈도를 value로 하는 pair RDD를 만듭니다.\n    .groupByKey                                         // key로 묶어서,\n    .mapValues(_.sum)                                   // value는 [1, 1, ..., 1] 이런 리스트일 것이므로 sum을 해주면 빈도가 계산됩니다.\n//    .reduceByKey(_ + _)                               // 한번에 이렇게도 됩니다. ^^;\n    .collect)",
      "dateUpdated": "Feb 26, 2016 12:49:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456457830467_-780237906",
      "id": "20160226-033710_817394039",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "survivedBySex: Array[((Any, Any), Int)] \u003d Array(((female,0),81), ((female,1),233), ((male,1),109), ((male,0),468))\n"
      },
      "dateCreated": "Feb 26, 2016 3:37:10 AM",
      "dateStarted": "Feb 26, 2016 12:49:20 PM",
      "dateFinished": "Feb 26, 2016 12:50:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n* 성별에 따라 우리는 아래와 같은 네가지 모델을 만들 수 있습니다.\n    - 남녀 불문하고 다 살린다.\n    - 남녀 불문하고 다 죽인다.\n    - 남자는 살리고 여자는 죽인다.\n    - 남자는 죽이고 여자는 살린다.\n* 위 결과를 토대로 남녀 각각 생존 확률은 아래와 같습니다.\n    - 여자는 233 / (81 + 233) \u003d 74.2%의 확률로 살아남았고,\n    - 남자는 109 / (109 + 468) \u003d 18.9%의 확률로 살아남았네요.\n* _레오나르도 디카프리오는 케이트 윈슬렛을 살리고 자신은 죽지요. 남자는 여자를 살리고 죽는가 봅니다._\n* 그러니 우리도 남자는 죽이고 여자는 살립시다! ㅠ.ㅠ",
      "dateUpdated": "Feb 26, 2016 12:49:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456459701165_1785349838",
      "id": "20160226-040821_1665909793",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cul\u003e\n\u003cli\u003e성별에 따라 우리는 아래와 같은 네가지 모델을 만들 수 있습니다.\u003cul\u003e\n\u003cli\u003e남녀 불문하고 다 살린다.\u003c/li\u003e\n\u003cli\u003e남녀 불문하고 다 죽인다.\u003c/li\u003e\n\u003cli\u003e남자는 살리고 여자는 죽인다.\u003c/li\u003e\n\u003cli\u003e남자는 죽이고 여자는 살린다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e위 결과를 토대로 남녀 각각 생존 확률은 아래와 같습니다.\u003cul\u003e\n\u003cli\u003e여자는 233 / (81 + 233) \u003d 74.2%의 확률로 살아남았고,\u003c/li\u003e\n\u003cli\u003e남자는 109 / (109 + 468) \u003d 18.9%의 확률로 살아남았네요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e레오나르도 디카프리오는 케이트 윈슬렛을 살리고 자신은 죽지요. 남자는 여자를 살리고 죽는가 봅니다.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e그러니 우리도 남자는 죽이고 여자는 살립시다! ㅠ.ㅠ\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 26, 2016 4:08:21 AM",
      "dateStarted": "Feb 26, 2016 12:49:10 PM",
      "dateFinished": "Feb 26, 2016 12:49:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// test 데이터를 읽어들입니다.\nval test \u003d (sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(\"/titanic/test.csv\"))\ntest.registerTempTable(\"test\")\ntest.cache",
      "dateUpdated": "Feb 26, 2016 12:49:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456461406203_265407792",
      "id": "20160226-043646_2141329586",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "test: org.apache.spark.sql.DataFrame \u003d [PassengerId: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]\nres12: test.type \u003d [PassengerId: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]\n"
      },
      "dateCreated": "Feb 26, 2016 4:36:46 AM",
      "dateStarted": "Feb 26, 2016 12:50:01 PM",
      "dateFinished": "Feb 26, 2016 12:50:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// 성별에 따라 살리고 죽입니다.\nval result \u003d (test.select(\"PassengerId\", \"Sex\")\n    .map(row \u003d\u003e (row(0), if (row(1) \u003d\u003d \"female\") 1 else 0))    // ㅠ.ㅠ\n    .collect)",
      "dateUpdated": "Feb 26, 2016 12:49:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456461483800_408088338",
      "id": "20160226-043803_1362898243",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "result: Array[(Any, Int)] \u003d Array((892,0), (893,1), (894,0), (895,0), (896,1), (897,0), (898,1), (899,0), (900,1), (901,0), (902,0), (903,0), (904,1), (905,0), (906,1), (907,1), (908,0), (909,0), (910,1), (911,1), (912,0), (913,0), (914,1), (915,0), (916,1), (917,0), (918,1), (919,0), (920,0), (921,0), (922,0), (923,0), (924,1), (925,1), (926,0), (927,0), (928,1), (929,1), (930,0), (931,0), (932,0), (933,0), (934,0), (935,1), (936,1), (937,0), (938,0), (939,0), (940,1), (941,1), (942,0), (943,0), (944,1), (945,1), (946,0), (947,0), (948,0), (949,0), (950,0), (951,1), (952,0), (953,0), (954,0), (955,1), (956,0), (957,1), (958,1), (959,0), (960,0), (961,1), (962,1), (963,0), (964,1), (965,0), (966,1), (967,0), (968,0), (969,1), (970,0), (971,1), (972,0), (973,0), (974,0), (975,0), (976,0)..."
      },
      "dateCreated": "Feb 26, 2016 4:38:03 AM",
      "dateStarted": "Feb 26, 2016 12:50:05 PM",
      "dateFinished": "Feb 26, 2016 12:50:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// 결과를 저장합니다.\nimport java.io.PrintWriter\n\nval resultStr \u003d Array(\"PassengerId,Survived\") ++ result.map(pair \u003d\u003e s\"${pair._1},${pair._2}\")\nnew PrintWriter(\"/titanic/gendermodel.csv\") { write(resultStr.mkString(\"\\n\")); close }",
      "dateUpdated": "Feb 26, 2016 12:49:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456461861540_1068609133",
      "id": "20160226-044421_351584153",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.io.PrintWriter\nresultStr: Array[String] \u003d Array(PassengerId,Survived, 892,0, 893,1, 894,0, 895,0, 896,1, 897,0, 898,1, 899,0, 900,1, 901,0, 902,0, 903,0, 904,1, 905,0, 906,1, 907,1, 908,0, 909,0, 910,1, 911,1, 912,0, 913,0, 914,1, 915,0, 916,1, 917,0, 918,1, 919,0, 920,0, 921,0, 922,0, 923,0, 924,1, 925,1, 926,0, 927,0, 928,1, 929,1, 930,0, 931,0, 932,0, 933,0, 934,0, 935,1, 936,1, 937,0, 938,0, 939,0, 940,1, 941,1, 942,0, 943,0, 944,1, 945,1, 946,0, 947,0, 948,0, 949,0, 950,0, 951,1, 952,0, 953,0, 954,0, 955,1, 956,0, 957,1, 958,1, 959,0, 960,0, 961,1, 962,1, 963,0, 964,1, 965,0, 966,1, 967,0, 968,0, 969,1, 970,0, 971,1, 972,0, 973,0, 974,0, 975,0, 976,0, 977,0, 978,1, 979,1, 980,1, 981,0, 982,1, 983,0, 984,1, 985,0, 986,0, 987,0, 988,1, 989,0, 990,1, 991,0, 992,1, 993,0, 994,0, 995,0, 996,1, 997,0, ...res18: java.io.PrintWriter \u003d $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anon$1@4f00fdba\n"
      },
      "dateCreated": "Feb 26, 2016 4:44:21 AM",
      "dateStarted": "Feb 26, 2016 12:50:06 PM",
      "dateFinished": "Feb 26, 2016 12:50:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nPart 2: Using MLlib\n-------------------",
      "dateUpdated": "Feb 26, 2016 12:49:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456462507139_-2010784790",
      "id": "20160226-045507_803944387",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003ePart 2: Using MLlib\u003c/h2\u003e\n"
      },
      "dateCreated": "Feb 26, 2016 4:55:07 AM",
      "dateStarted": "Feb 26, 2016 12:49:10 PM",
      "dateFinished": "Feb 26, 2016 12:49:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// 데이터를 벡터로 표현합니다.\n// 참조: http://spark.apache.org/docs/latest/mllib-data-types.html\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.mllib.regression.LabeledPoint\n\n// 문자열 컬럼의 첫번째 문자를 반환하는 함수입니다. 빈 문자열의 경우 \u0027_\u0027를 반환합니다.\ndef firstChar(col: Any): Char \u003d if (col.toString.length \u003e 0) col.toString()(0) else \u0027_\u0027\n\n// 문자열인 \"Cabin\" 컬럼의 값을 숫자로 변환하기 위한 맵입니다. (첫 문자) -\u003e (숫자)\nval cabinMap \u003d train.select(\"Cabin\").map(row \u003d\u003e firstChar(row(0))).collect.toSet.toList.zipWithIndex.toMap\n\n// 문자열인 \"Embarked\" 컬럼의 값을 숫자로 변환하기 위한 맵입니다. (첫 문자) -\u003e (숫자)\nval embarkedMap \u003d train.select(\"Embarked\").map(row \u003d\u003e firstChar(row(0))).collect.toSet.toList.zipWithIndex.toMap\n\n// 컬럼의 값을 Double 형으로 반환하는 함수입니다.\ndef toDbl(col: Any): Double \u003d col.toString.toDouble\n\n// 하나의 row(features)를 벡터로 표현하는 함수입니다.\ndef makeVector(row: org.apache.spark.sql.Row): Vector \u003d {\n    def cabin(col: Any): Double \u003d cabinMap(firstChar(col))\n    def embarked(col: Any): Double \u003d embarkedMap(firstChar(col))\n    \n    Vectors.dense(\n        toDbl(row(0)),    // Pclass\n        if (row(1) \u003d\u003d \"female\") 1 else 0,    // Sex\n        toDbl(row(2)),    // Age\n        toDbl(row(3)),    // SibSp\n        toDbl(row(4)),    // Parch\n        toDbl(row(5)),    // Fare\n        cabin(row(6)),    // Cabin\n        embarked(row(7))    // Embarked\n    )\n}\n\n// train 데이터를 읽어들여 LabeledPoint 형태(label, vector pair)로 변환합니다.\nval trainLblPnt \u003d (train.select(\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\", \"Survived\")\n    .map(row \u003d\u003e LabeledPoint(toDbl(row(8)), makeVector(row))))",
      "dateUpdated": "Feb 26, 2016 12:49:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456474246653_1488161781",
      "id": "20160226-081046_1134617503",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.mllib.regression.LabeledPoint\nfirstChar: (col: Any)Char\ncabinMap: scala.collection.immutable.Map[Char,Int] \u003d Map(E -\u003e 0, T -\u003e 1, F -\u003e 2, A -\u003e 3, G -\u003e 4, B -\u003e 5, _ -\u003e 6, C -\u003e 7, D -\u003e 8)\nembarkedMap: scala.collection.immutable.Map[Char,Int] \u003d Map(S -\u003e 0, C -\u003e 1, Q -\u003e 2, _ -\u003e 3)\ntoDbl: (col: Any)Double\nmakeVector: (row: org.apache.spark.sql.Row)org.apache.spark.mllib.linalg.Vector\ntrainLblPnt: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] \u003d MapPartitionsRDD[40] at map at \u003cconsole\u003e:45\n"
      },
      "dateCreated": "Feb 26, 2016 8:10:46 AM",
      "dateStarted": "Feb 26, 2016 12:50:08 PM",
      "dateFinished": "Feb 26, 2016 12:50:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Random Forest를 이용하여 학습 및 평가를 진행합니다.\n// 참조: http://spark.apache.org/docs/latest/mllib-ensembles.html\n\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\n\n// 학습 데이터를 7:3으로 나눠 7에서 학습하고 3으로 평가를 진행합니다.\nval splits \u003d trainLblPnt.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) \u003d (splits(0), splits(1))\n\n// 학습 파라미터를 설정합니다.\nval numClasses \u003d 2                               // 출력은 죽은 경우(0)와 산 경우(1)로 두개입니다.\nval categoricalFeaturesInfo \u003d Map[Int, Int]()    // Pclass, Sex, SibSp, Parch, Cabin, Embarked는 연속된 숫자가 아니라 categorical feature 입니다.\nval numTrees \u003d 3                                 // decision tree를 몇개 만들 지 지정합니다.\nval featureSubsetStrategy \u003d \"auto\"               // 자동이 있으니 그냥 쓰시죠. ^^;\nval impurity \u003d \"gini\"                            // 잘 모르겠네요. ㅠ.ㅠ\nval maxDepth \u003d 4                                 // decision tree의 깊이를 말하는 듯 한데..\nval maxBins \u003d 32                                 // decision tree는 categorical feature 기반이므로 Age 같은 숫자(continuous feature)는 구간으로 나눠야 합니다.\n                                                 // 그 구간을 나눌 때 최대 몇개까지로 지정할 지를 말하는 것 같습니다. ㅎㅎ",
      "dateUpdated": "Feb 26, 2016 12:56:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456464915010_-758103439",
      "id": "20160226-053515_675029747",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] \u003d Array(MapPartitionsRDD[41] at randomSplit at \u003cconsole\u003e:48, MapPartitionsRDD[42] at randomSplit at \u003cconsole\u003e:48)\ntrainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] \u003d MapPartitionsRDD[41] at randomSplit at \u003cconsole\u003e:48\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] \u003d MapPartitionsRDD[42] at randomSplit at \u003cconsole\u003e:48\nnumClasses: Int \u003d 2\ncategoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] \u003d Map()\nnumTrees: Int \u003d 3\nfeatureSubsetStrategy: String \u003d auto\nimpurity: String \u003d gini\nmaxDepth: Int \u003d 4\nmaxBins: Int \u003d 32\n"
      },
      "dateCreated": "Feb 26, 2016 5:35:15 AM",
      "dateStarted": "Feb 26, 2016 12:50:09 PM",
      "dateFinished": "Feb 26, 2016 12:50:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n이 뒤로는 [이곳](http://spark.apache.org/docs/latest/mllib-ensembles.html)을 보며 각자 한번 해봅시다. ^^;",
      "dateUpdated": "Feb 26, 2016 12:57:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456474903282_1414826338",
      "id": "20160226-082143_1445959633",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e이 뒤로는 \u003ca href\u003d\"http://spark.apache.org/docs/latest/mllib-ensembles.html\"\u003e이곳\u003c/a\u003e을 보며 각자 한번 해봅시다. ^^;\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 26, 2016 8:21:43 AM",
      "dateStarted": "Feb 26, 2016 12:57:39 PM",
      "dateFinished": "Feb 26, 2016 12:57:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456491385272_23397151",
      "id": "20160226-125625_2061129514",
      "dateCreated": "Feb 26, 2016 12:56:25 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Kaggle 시작하기",
  "id": "2BBKQHABY",
  "angularObjects": {
    "2BCXT4EMZ": [],
    "2BDUUR8VM": [],
    "2BD1WSCUT": [],
    "2BBSKQMQP": [],
    "2BD8KU4ZX": [],
    "2BCNX7XJ5": [],
    "2BCFCM4H2": [],
    "2BES5RH3E": [],
    "2BBMK65YZ": [],
    "2BETWU73M": [],
    "2BCH33WQT": [],
    "2BBJXF9PM": [],
    "2BEJ61NTA": [],
    "2BEUC6XBN": [],
    "2BD6YJVY5": []
  },
  "config": {},
  "info": {}
}